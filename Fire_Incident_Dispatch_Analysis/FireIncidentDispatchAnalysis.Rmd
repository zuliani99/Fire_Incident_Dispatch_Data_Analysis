---
title: "Fire Incident Dispatch Data Analysis"
author: "Zuliani Riccardo"
date: "12/12/2023"
output: 
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE) #, cache=TRUE
setwd("C:/Users/ricca/Desktop/UNI/Magistrale/Anno3/Statistical_Inference_and_Learning/SIL Projcet/Statistical_Inference_Learning_Project/Fire_Incident_Dispatch_Analysis")
```

# Loads & Install Packages

```{r including library}
if (!require("nnet")) install.packages("nnet")
if (!require("MASS")) install.packages("MASS")
if (!require("e1071")) install.packages("e1071")
if (!require("class")) install.packages("class")
if (!require("leaps")) install.packages("leaps")
if (!require("glmnet")) install.packages("glmnet")
if (!require("car")) install.packages("car")
if (!require("caTools")) install.packages("caTools")

if (!require("summarytools")) install.packages("summarytools")
if (!require("dplyr")) install.packages("dplyr")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("lubridate")) install.packages("lubridate")
if (!require("mapview")) install.packages("mapview")
if (!require("sf")) install.packages("sf")
if (!require("geojsonio")) install.packages("geojsonio")
if (!require("leaflet")) install.packages("leaflet")
if (!require("broom")) install.packages("broom")
if (!require("plotly")) install.packages("plotly")

library(nnet)
library(MASS)
library(e1071)
library(class)
library(leaps)
library(glmnet)
library(car)
library(caTools)

library(summarytools)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(lubridate)
library(mapview)
library(sf)
library(geojsonio)
library(leaflet) 
library(broom)
library(plotly)
```


# Dataset description

The Fire Incident Dispatch Data file contains data that is generated by the Starfire Computer Aided Dispatch System. The data spans from the time the incident is created in the system to the time the incident is closed in the system. It covers information about the incident as it relates to the assignment of resources and the Fire Departmentâ€™s response to the emergency. To protect personal identifying information in accordance with the Health Insurance Portability and Accountability Act (HIPAA), specific locations of incidents are not included and have been aggregated to a higher level of detail.

In this analysis we have restricted the analysis only on the *last 50.000 observations* from 5th of September to 30th of the same month. 

1. **STARFIRE_INCIDENT_ID**: An incident identifier comprising the 5 character julian date, 4 character alarm box number, 2 character number of incidents at the box so far for the day, 1 character borough code , 4 character sequence number. 
2. **INCIDENT_DATETIME**: The date and time of the incident.
3. **ALARM_BOX_BOROUGH**: The borough of the alarm box.
4. **ALARM_BOX_LOCATION**: The location of the alarm box.
5. **ALARM_BOX**: The alarm box number.
6. **INCIDENT_BOROUGH**: The borough of the incident.
7. **ZIPCODE**: The zip code of the incident.
8. **POLICEPRECINCT**: The police precinct of the incident.
9. **CITYCOUNCILDISTRICT**: The city council district.
10. **COMMUNITYDISTRICT**: The community district.
11. **COMMUNITYSCHOOLDISTRICT**: The community school district.
12. **CONGRESSIONALDISTRICT**: The congressional district.
13. **ALARM_SOURCE_DESCRIPTION_TX**: The description of the alarm source.
14. **ALARM_LEVEL_INDEX_DESCRIPTION**: The alarm level index.
15. **HIGHEST_ALARM_LEVEL**: The highest alarm level.
16. **INCIDENT_CLASSIFICATION**: The incident classification.
17. **INCIDENT_CLASSIFICATION_GROUP**: The incident classification roll up group.

18. **FIRST_ASSIGNMENT_DATETIME**: The date and time of the first unit assignment.
19. **FIRST_ACTIVATION_DATETIME**: The date and time of the first unit acknowledgement of the assignment.
20. **FIRST_ON_SCENE_DATETIME**: The date and time of the first unit at the scene of the incident.
21. **INCIDENT_CLOSE_DATETIME**: The date and time that the incident was closed in the dispatch system.

22. **VALID_DISPATCH_RSPNS_TIME_INDC**: Indicates that the components comprising the generation of the DISPATCH_RESPONSE_SECONDS_QY are valid.
23. **DISPATCH_RESPONSE_SECONDS_QY**: The elapsed time in seconds between the incident_datetime and the first_assignment_datetime.

24. **VALID_INCIDENT_RSPNS_TIME_INDC**: Indicates that the components comprising the generation of the INCIDENT_RESPONSE_SECONDS_QY are valid.
25. **INCIDENT_RESPONSE_SECONDS_QY**: The elapsed time in seconds between the incident_datetime and the first_onscene_datetime.

26. **INCIDENT_TRAVEL_TM_SECONDS_QY**: The elapsed time in seconds between the first_assignment_datetime and the first_onscene_datetime.


27. **ENGINES_ASSIGNED_QUANTITY**: The number of engine units assigned to the incident.
28. **LADDERS_ASSIGNED_QUANTITY**: The number of ladder units assigned to the incident.
29. **OTHER_UNITS_ASSIGNED_QUANTITY**: The number of  units that are not engines or ladders that were assigned to the incident.


Regarding the response we will create two different analysis one with the aim to predict the **INCIDENT_RESPONSE_SECONDS_QY** and the other to predict the **TICKET_TIME_QY**. In both models the remaining time difference predictors are removed.


# Data Exlporation and Cleaning

The first step is always to read the dataset and plot the first 5 observations
```{r read dataset, cache=TRUE}
fire_data <- read.csv("datasets/Fire_Incident_Dispatch_Data_last_50k.csv")

head(fire_data)
```

Use `dfSummary` from `summarytool` in order to have a complete and clear sumamry of the dataset.
```{r print datset}
print(dfSummary(fire_data, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE),
                method = 'render')
```

Now we rename all the columns in order to be smaller whenever we plot graphs.
```{r predictor renaming}
fire_data <- fire_data %>%
            rename(id = STARFIRE_INCIDENT_ID, datetime = INCIDENT_DATETIME, al_borough = ALARM_BOX_BOROUGH,
                   al_number = ALARM_BOX_NUMBER,al_location = ALARM_BOX_LOCATION, inc_borough = INCIDENT_BOROUGH,
                   zipcode = ZIPCODE, pol_prec = POLICEPRECINCT, city_con_dist = CITYCOUNCILDISTRICT,
                   commu_dist = COMMUNITYDISTRICT, commu_sc_dist = COMMUNITYSCHOOLDISTRICT,
                   cong_dist = CONGRESSIONALDISTRICT, al_source_desc = ALARM_SOURCE_DESCRIPTION_TX,
                   al_index_desc = ALARM_LEVEL_INDEX_DESCRIPTION, highest_al_level = HIGHEST_ALARM_LEVEL,
                   inc_class = INCIDENT_CLASSIFICATION, inc_class_group = INCIDENT_CLASSIFICATION_GROUP,
                   first_ass_datetime = FIRST_ASSIGNMENT_DATETIME, first_act_datetime = FIRST_ACTIVATION_DATETIME,
                   first_onscene_datetime = FIRST_ON_SCENE_DATETIME, inc_close_datetime = INCIDENT_CLOSE_DATETIME, 
                   
                   disp_resp_sec_qy = DISPATCH_RESPONSE_SECONDS_QY, disp_resp_sec_indc = VALID_DISPATCH_RSPNS_TIME_INDC,
                   inc_resp_sec_qy = INCIDENT_RESPONSE_SECONDS_QY, inc_resp_sec_indc = VALID_INCIDENT_RSPNS_TIME_INDC,
                   
                   inc_travel_sec_qy = INCIDENT_TRAVEL_TM_SECONDS_QY, 
                   
                   engines_assigned = ENGINES_ASSIGNED_QUANTITY,
                   ladders_assigned = LADDERS_ASSIGNED_QUANTITY, others_units_assigned = OTHER_UNITS_ASSIGNED_QUANTITY)
```


As we can see from the summary there are many `NA` values, and many predictors that are as characters and not factors. In this step we will convert the characters predictors as factors merging the values that appear less in the dataset, so we do no have many values that have low frequency in our dataset.

In addition we will add he predictor for the `day_number`, a factorial predictor to indicate in the incident day is a week day or not `dat_type` and a factorial predictor `time_of_day` that indicates the range of time whenever the incident happens, so `Night` (if the hour is between 0 and 6), `Morning`  (if the hour is between 6 and 12), `Afternoon` (if the hour is between 12 and 18), `Evening`  (if the hour is between 18 and 24).

Since we are dealing with datetime we also check if the differences (`inc_resp_sec_qy`, `inc_travel_sec_qy` and `disp_resp_sec_qy`) are actually corrects, if not we replace them with the correct one.

Finally we decided to add an additional time difference the `emergency_min_qy` which represents the difference between the `inc_close_datetime` and the `first_onscene_datetime`.


```{r factorial type}
# set factorial
fire_data$inc_borough <- as.factor(fire_data$inc_borough)
fire_data$al_borough <- as.factor(fire_data$al_borough)
fire_data$al_source_desc <- as.factor(fire_data$al_source_desc)
fire_data$al_index_desc <- as.factor(fire_data$al_index_desc)
fire_data$highest_al_level <- as.factor(fire_data$highest_al_level)

fire_data$disp_resp_sec_indc <- as.factor(fire_data$disp_resp_sec_indc)
levels(fire_data$disp_resp_sec_indc)<- c("N", "Y")

fire_data$inc_resp_sec_indc <- as.factor(fire_data$inc_resp_sec_indc)
levels(fire_data$inc_resp_sec_indc)<- c("N", "Y")

fire_data$inc_class_group <- as.factor(fire_data$inc_class_group)
fire_data$inc_class <- as.factor(fire_data$inc_class)
```

Moreover we note that the maximum level of the time differences is very high to be considered as seconds so we decided to scale the two indicators in minutes.

```{r summary datetime predictors}
summary(fire_data %>% select(inc_resp_sec_qy, inc_travel_sec_qy, disp_resp_sec_qy))
```

```{r scaling datetime to minutes}
# scaling
fire_data$inc_resp_sec_qy <- fire_data$inc_resp_sec_qy / 60
fire_data$inc_travel_sec_qy <- fire_data$inc_travel_sec_qy / 60
fire_data$disp_resp_sec_qy <- fire_data$disp_resp_sec_qy / 60 

# renaming both quantity and indicator predictors for the two datetime 
fire_data <- fire_data %>% rename(inc_resp_min_qy = inc_resp_sec_qy, inc_travel_min_qy = inc_travel_sec_qy, disp_resp_min_qy = disp_resp_sec_qy, # quantity
                                  inc_resp_min_indc = inc_resp_sec_indc, disp_resp_min_indc = disp_resp_sec_indc) # indicator
```


Here we create the `time_of_day` and `is_weekend`

```{r adding custom datetime predictors}
# Process datetime column
fire_data$datetime <- mdy_hms(fire_data$datetime)
fire_data$first_ass_datetime <- mdy_hms(fire_data$first_ass_datetime)
fire_data$first_act_datetime <- mdy_hms(fire_data$first_act_datetime)
fire_data$first_onscene_datetime <- mdy_hms(fire_data$first_onscene_datetime)
fire_data$inc_close_datetime <- mdy_hms(fire_data$inc_close_datetime)


# checking if the differences are well computed if not change with the correct one

if (!identical(fire_data$inc_resp_min_qy, as.numeric(difftime(fire_data$first_onscene_datetime, fire_data$datetime, units="mins")))){
  fire_data$inc_resp_min_qy <- as.numeric(difftime(fire_data$first_onscene_datetime, fire_data$datetime, units="mins"))
}

if (!identical(fire_data$inc_travel_min_qy, as.numeric(difftime(fire_data$first_onscene_datetime, fire_data$first_ass_datetime, units="mins")))){
  fire_data$inc_travel_min_qy <- as.numeric(difftime(fire_data$first_onscene_datetime, fire_data$first_ass_datetime, units="mins"))
}

if (!identical(fire_data$disp_resp_min_qy, as.numeric(difftime(fire_data$first_ass_datetime, fire_data$datetime, units="mins")))){
  fire_data$disp_resp_min_qy <- as.numeric(difftime(fire_data$first_ass_datetime, fire_data$datetime, units="mins"))
}

# creating emergency_min_qy which describe the time taken by the firefighter to close the emergency after have been arrived to the location 
fire_data$emergency_min_qy <- as.numeric(difftime(fire_data$inc_close_datetime, fire_data$first_onscene_datetime, units="mins"))

# creating day_type
fire_data$day_type <- as.factor(ifelse(weekdays(fire_data$datetime) %in% c("sabato", "domenica"), "Weekend", "Weekday"))

# creating ticket_time
fire_data$ticket_time <- as.numeric(difftime(fire_data$inc_close_datetime, fire_data$datetime, units="mins"))
  
# creating time_of_day
fire_data$time_of_day <- cut(
    hour(fire_data$datetime),
    breaks = c(0, 6, 12, 18, 24),
    labels = c("Night", "Morning", "Afternoon", "Evening"),
    include.lowest = TRUE,
    right = TRUE
)
  
fire_data$datetime <- NULL
```


```{r table time_of_day}
table(fire_data$time_of_day)
```
```{r geom_bar time_of_day}
ggplot(data=fire_data %>% 
          group_by(time_of_day) %>%
          summarise(incident_number = n()), 
        aes(x=time_of_day, y=incident_number)) + 
      geom_bar(stat="identity", position=position_dodge()) +
      geom_text(aes(label=incident_number), vjust=1.6, color="white", position = position_dodge(0.9), size=3.5) +
      labs(title = "Time of the Day - Incident Count", x = "Time of the Day", y = "Incident Count") +
      theme_grey()
```

From this we can see that the higher number of fire incident is registered from 12 PM to 18 PM, whereas the lower number of fire incident happened from the 00 AM to 06 AM.


```{r table day_type}
day_type_table <- table(fire_data$day_type)
day_type_table[1] <- day_type_table[1] / 5
day_type_table[2] <- day_type_table[2] / 2
day_type_table
```

And in proportion we can see that on average there is an higher number of fire incident on the week day respect to the week end days.


Now regarding the `assigned untis` we decided to add a summary predictor that include the sum of all the three assigned units predictors.

```{r total_assigned_unit}
fire_data$total_assigned_unit <- fire_data$engines_assigned + fire_data$ladders_assigned + fire_data$others_units_assigned
```



Rename the factor levels for the `inc_borough` and  predictors

```{r changing levels for inc_borough and al_borough}
fire_data <- fire_data %>% mutate(inc_borough = recode_factor(
                  inc_borough, "BRONX" = "Bronx", "BROOKLYN" = "Brooklyn", "MANHATTAN" = "Manhattan",
                  "QUEENS" = "Queens", "RICHMOND / STATEN ISLAND" = "Staten Island"),
                  
                  al_borough = recode_factor(
                  al_borough, "BRONX" = "Bronx", "BROOKLYN" = "Brooklyn", "MANHATTAN" = "Manhattan",
                  "QUEENS" = "Queens", "RICHMOND / STATEN ISLAND" = "Staten Island"))
```



At this point we merge some possible value from factorial predictors to make the space of possible choice smaller.

Here we merge the following factorial values of `highest_al_level`: `Second Alarm` and `Third Alarm` into `2nd-3rd Alarm`.


```{r merging factors for highest_al_level}
# highest_al_level
fire_data$highest_alarm_lev_new <- fire_data$highest_al_level
levels(fire_data$highest_alarm_lev_new) <- list(
  "All Hands Working" = "All Hands Working",
  "First Alarm" = "First Alarm", 
  "2nd-3rd Alarm" = c("Second Alarm", "Third Alarm")
)

print(ctable(fire_data$highest_al_level, fire_data$highest_alarm_lev_new, prop = 'n', totals = FALSE, headings = FALSE), method = 'render')

fire_data$highest_al_level <- fire_data$highest_alarm_lev_new
fire_data$highest_alarm_lev_new <- NULL
```

Here we merge the following factorial values of `al_index_desc`: `Second Alarm`, `Third Alarm`, `7-5 (All Hands Alarm)`, `10-76 & 10-77 Signal (Notification Hi-Rise Fire)` and `10-75 Signal (Request for all hands alarm)` into `Others`.

```{r merging factors for al_index_desc}
# al_index_desc
fire_data$alarm_level_idx_new <- fire_data$al_index_desc
levels(fire_data$alarm_level_idx_new) <- list(
  "DEFAULT RECORD" = "DEFAULT RECORD",
  "Initial Alarm" = "Initial Alarm", 
  "Others" = c("Second Alarm", "Third Alarm", "7-5 (All Hands Alarm)", 
               "10-76 & 10-77 Signal (Notification Hi-Rise Fire)",
               "10-75 Signal (Request for all hands alarm)")
)

print(ctable(fire_data$al_index_desc, fire_data$alarm_level_idx_new, prop = 'n', totals = FALSE, headings = FALSE), method = 'render')

fire_data$al_index_desc <- fire_data$alarm_level_idx_new
fire_data$alarm_level_idx_new <- NULL
```

Here we merge the following factorial values of `al_source_desc`: `911`, `911TEXT`, `VERBAL`, `BARS`, `ERS`, `ERS-NC` and `SOL` into `Others`.

```{r merging factors for al_source_desc}
fire_data$alarm_source_desc_new <- fire_data$al_source_desc
levels(fire_data$alarm_source_desc_new) <- list(
  "PHONE" = "PHONE",
  "EMS" = "EMS",
  "EMS-911" = "EMS-911",
  "CLASS-3" = "CLASS-3",
  "Others" = c("911", "911TEXT", "VERBAL", "BARS", "ERS", "ERS-NC", "SOL")
)

print(ctable(fire_data$al_source_desc, fire_data$alarm_source_desc_new, prop = 'n', totals = FALSE, headings = FALSE), method = 'render')

fire_data$al_source_desc <- fire_data$alarm_source_desc_new
fire_data$alarm_source_desc_new <- NULL
```

View again the dataset summary to see the applied changes.

```{r print updated datset}
print(dfSummary(fire_data, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE),
                method = 'render')
```






## Dealing with invalid values
The next step is to deal invalid values and delete some un-useful predictors.


First of all we saw the possibility that `al_borough` and `inc_borough` represent the same column, let's chek it.


```{r identical al_borough - inc_borough}
identical(fire_data$al_borough, fire_data$inc_borough)
```
The column ``al_borough` and `inc_borough` have the same sequence of values, so we can delete one of the two.

```{r remove al_borough}
fire_data <- fire_data %>% select(-c(al_borough))
```



Then we say that all observation in the dataset have the `disp_resp_min_indc` equal to *N*, let's check again and in affermative case then we can delete both columns.

```{r sumamry disp_resp_min_indc}
summary(fire_data$disp_resp_min_indc)
```


All our observations have non valid `disp_resp_min_indc` so we could delete both the column indicator and the respective column quantity `disp_resp_min_qy`. However we note that also in the original dataset all the observation have the `disp_resp_min_indc` set to **N**, which is quite strange, and seems that is problem relative to the data acquisition, thus we decide to mantein this time difference.

```{r remove al_borough}
fire_data <- fire_data %>% select(-c(disp_resp_min_indc))
```


Now we do a quick check also on the other indicator variable `inc_resp_min_indc`

```{r sumamry inc_resp_min_indc}
summary(fire_data$inc_resp_min_indc)
```

But here we have some observations with valid `inc_resp_min_indc`, and we will consider only the valid one deleting the one that has a non valid attribute.

However before doing that let's see the distribution of `inc_resp_min_qy` around the borough.

```{r ggplot inc_resp_min_qy}
ggplot(data=fire_data %>% group_by(inc_borough, inc_resp_min_indc) %>% summarise(incident_number = n()), 
       aes(x=inc_borough, y=incident_number, fill=inc_resp_min_indc)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=incident_number), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5) +
  scale_fill_brewer(palette="Paired") +
  labs(title = "Incident Count - Borouh - Valid Response Time in Minutes", x = "Borough", y = "Incident Number", fill = "Valid Response\n Time in Minutes") +
  theme_gray()
```

We can see that the number of fire incident is higher for the valid response time in minutes but, it is much interesting observe the rateo between the valid and the non valid.

And to the rateo of valid `inc_resp_min_indc` in each borough is:

```{r ggplot rateo incident resp_indc}
rateo_inc_resp_min_indc <- fire_data %>% 
  group_by(inc_borough, inc_resp_min_indc) %>% 
  summarise(incident_number = n()) %>% 
  mutate(ratio=incident_number/sum(incident_number))

ggplot(rateo_inc_resp_min_indc, aes(fill=inc_resp_min_indc, y=ratio, x=inc_borough)) + 
  geom_bar(position="fill", stat="identity") + 
  geom_text(aes(label=scales::percent(ratio)), position=position_fill(vjust=0.5)) +
  labs(title="Borough - Rateo Incident between Valid and Invalid",
       x="Borough",
       y="Rateo Incident between Valid and Invalid",
       fill="Valid Response\nTime in Minutes")

```

And we can see that Staten Island has the higher number of incidents with valid `inc_resp_min_indc` , whereas Manhattan has the lower number, but remember that the former has the lowest number of fire incident and the latter has the higher number of incident.

Now we do an additional analysis to see if there is some find of relation between the `inc_resp_min_indc` and `total_assigned_unit`.

```{r geom_point tau resp_qy}
ggplot(fire_data, aes(total_assigned_unit, inc_resp_min_qy)) + 
  geom_point(aes(colour = inc_resp_min_indc))+
   labs(title = "Total Assigned Units - Response Time In Minutes", x = "Total Assigned Units", y = "Response Time In Minutes", colour = "Valid Response\n Time in Minutes") +
  theme_gray()
```

We note that the majority of fire incident that had been assigned a single units has a high response time and the relative measure is not valid. Whereas for an higher number of total units the response time decrease and becomes valids.


```{r geom_point res_indc N tau resp_qy}
ggplot(fire_data %>% filter(inc_resp_min_indc == "N")
            , aes(total_assigned_unit, inc_resp_min_qy)) + 
  geom_point(aes(colour = inc_class_group)) +
  labs(title = "Total Assigned Units - Response Time In Minutes - Incidnet Class Group", x = "Total Assigned Units", y = "Response Time In Minutes", colour = "Incident Class Groups") +
  theme_gray()
```

Regarding the incident class group around all the incidents with invalid response time had been assigned a single units as we discussed before, but in addition we found that are from the **Medical Emergencies**, whereas almost all the other incidents are from the **NonMedical Emergencies**.


```{r geom_bar tua_is_one}
# add an additional predictor
fire_data$tua_is_one <- as.factor(ifelse(fire_data$total_assigned_unit == 1, "Y", "N"))
  
tua_is_one <- fire_data %>% 
        filter(inc_resp_min_indc == "N", inc_class_group == "Medical Emergencies") %>%
        group_by(inc_borough, tua_is_one) %>%
        summarise(incident_number = n())

ggplot(data=tua_is_one, 
       aes(x=inc_borough, y=incident_number, fill=tua_is_one)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=incident_number), vjust=1.5, color="black",
            position = position_dodge(0.9), size=3.5) +
  scale_fill_brewer(palette="Set1") +
  labs(title = "Total Assigned Units One or Not", x = "Borough", y = "Incident Count", fill = "Total Assigned\nUnits are One") +
  theme_gray()
```

We have also added an additional factorial predictor `tua_is_one` to indicates if the total assigned units is equal to one or not.


Continuing we decide to analyse the type of Incident Class of the invalid incidents response time that had been assigned a single total units.

```{r geom_bar tua_is_one ME inc_class, fig.height = 5, fig.width = 15}
ggplot(data=fire_data %>% 
        filter(inc_resp_min_indc == "N", inc_class_group == "Medical Emergencies", tua_is_one == "Y") %>%
        group_by(inc_class, inc_borough) %>%
        summarise(incident_number = n()), 
       aes(x=inc_borough, y=incident_number, fill=inc_class)) + 
      geom_bar(stat="identity", position=position_dodge()) +
        geom_text(aes(label=incident_number), vjust=1.6, color="black",
                  position = position_dodge(0.9), size=3) +
        scale_fill_brewer(palette="Set1") +
        labs(title = "Borough - Incident Counts - Incident Class -- for Total Assigned Units equal to 1", x = "Borough", y = "Incident Counts", fill = "Incident Class Group") +
        theme_grey()
```


And we found that the majority of the incident that respect these circumstances are mostly identified as **Medical - EMS Link 10-91** and **Medical - PD Link 10-91**.

Thanks to the [10code](http://www.fdnewyork.com/10code.asp) site we found a description of the two emergency codes:

1. **10-91 Medical Emergency EMS** - Fire Unit Not Required - To be transmitted through borough dispatcher by the responding unit when the fire Unit is canceled enroute due to EMS on scene, or EMS downgrades the job to a segment that does not require a Fire Unit response. Note: This signal shall be used only for medical emergency incidents. EMS  we are sure that stands for *Emergency Medical Services*.

2. **10-91 Medical Emergency PD** - Fire Unit Not Required - To be transmitted through borough dispatcher by the responding unit when the fire Unit is canceled enroute due to PD on scene, or PD downgrades the job to a segment that does not require a Fire Unit response. Note: This signal shall be used only for medical emergency incidents. PD we think that stands for *Police Department*.



Now we can look for the NonMedical Emergencies by first see the distribution of its incident class.


```{r dist non resp_indc for NME}
print(fire_data %>% 
        filter(inc_resp_min_indc == "N", inc_class_group == "NonMedical Emergencies") %>%
        group_by(inc_class) %>%
        summarise(incident_number = n()))
```
```{r geom_bar borough N resp_indc for NME}
ggplot(data=fire_data %>% 
          filter(inc_resp_min_indc == "N", inc_class_group == "NonMedical Emergencies", inc_class == "Assist Civilian - Non-Medical") %>%
          group_by(inc_borough) %>%
          summarise(incident_number = n()), 
        aes(x=inc_borough, y=incident_number)) + 
      geom_bar(stat="identity", position=position_dodge()) +
      geom_text(aes(label=incident_number), vjust=1.6, color="white", position = position_dodge(0.9), size=3.5) +
      #scale_fill_brewer(palette="Paired") +
      labs(title = "Incident Count - Borouh - Valid Response Time in Second", x = "Borough", y = "Incident Count") +
      theme_grey()
```

And we found that the majority of non valid `inc_resp_min_indc` that are Non-Medical Emergency are from the incident class equal to Assist Civilian - Non-Medical.



For stake of consistency we will consider only the valid observations that have `inc_resp_min_indc == "Y"`.

```{r remove N resp_indc}
fire_data <- fire_data %>% filter(inc_resp_min_indc == "Y")
dim(fire_data)
```


Now we want to know how many `inc_class` are summarized in each `inc_class_group`, to be sure that each `inc_class_group` is referred to a single `inc_class`.

```{r ctable for inc_class}
print(ctable(fire_data$inc_class, fire_data$inc_class_group, totals = FALSE, headings = FALSE), method = 'render')
```

As we can see from the upper table all the `inc_class_group` have a unique set of values.

At this point to be more clear we display each main class with each respective sub-class.

```{r count inc_class per inc_class_group}
for (variable in levels(fire_data$inc_class_group)) {
  non_zero_table <- table(subset(fire_data, inc_class_group == variable)$inc_class)
  cat(variable, "\n")
  print(non_zero_table[non_zero_table != 0])
  cat("\n")
}
```


## NA Patterns?

At this point is essential to deal with NA values, trying to find the presence of possible relation with predictors.
First things first let's recap the number of NA values for each columns that we have at the moment.

```{r is.na}
colSums(is.na(fire_data))
```

### Checking the location predictors

Here we will check if there is a pattern on the absence of values in the following predictors: `zipcode`, `pol_prec`, `city_con_dist`, `commu_dist`, `commu_sc_dist` and `cong_dist`.

```{r na_locations}
na_locations <- fire_data %>%
  filter(is.na(zipcode) | is.na(pol_prec) | is.na(city_con_dist) | is.na(commu_dist) | is.na(commu_sc_dist) | is.na(cong_dist))
```


```{r geom_bar na_locations, fig.height = 5, fig.width = 10}
ggplot(data=na_locations %>% 
        group_by(inc_class_group, inc_borough) %>%
        summarise(incident_number = n()), 
       aes(x=inc_borough, y=incident_number, fill=inc_class_group)) + geom_bar(stat="identity", position=position_dodge()) +
        geom_text(aes(label=incident_number), vjust=1.6, color="black",
                  position = position_dodge(0.9), size=3.5) +
        #scale_fill_brewer(palette="Paired") +
        labs(title = "NA location", x = "Borough", y = "Incident Count", fill = "Incident Class Group") +
        theme_grey()
```


By the Bar Chart we note that the majority of observations that have at least one of the location predictors to NA are of the incident class group NonMedical Emergency, Non Medical MFAs and Medical Emergencies

```{r rateo na_locations per borough}
table(na_locations$inc_borough) / table(fire_data$inc_borough)
```

```{r rateo na_locations per inc_class_group}
table(na_locations$inc_class_group) / table(fire_data$inc_class_group)
```

Moreover around the 40% of the whole incident that are of the incident class group NonMedical MFAs have at least one of the location columns to NA. Let's investigate.


```{r analysis NonMedical MFAs 1}
fd_nm_mfa_cl <- table(subset(fire_data, inc_class_group == "NonMedical MFAs")$inc_class)
fd_nm_mfa_bro <- table(subset(fire_data, inc_class_group == "NonMedical MFAs")$inc_borough)

fd_nm_mfa_cl <- fd_nm_mfa_cl[fd_nm_mfa_cl != 0]
fd_nm_mfa_cl
```

In the original dataset this is the distribution of inc_class for the NonMedical MF

```{r analysis NonMedical MFAs 2}
na_nm_mfa_cl <- table(subset(na_locations, inc_class_group == "NonMedical MFAs")$inc_class)
na_nm_mfa_bro <- table(subset(na_locations, inc_class_group == "NonMedical MFAs")$inc_borough)

na_nm_mfa_cl <- na_nm_mfa_cl[names(fd_nm_mfa_cl)]
na_nm_mfa_cl
```



```{r rateo for NonMedical MFAs 1}
na_nm_mfa_cl / fd_nm_mfa_cl
```
So the 97% of all the Non-Medical MFA - ERS observations in the entire dataset have one of the location attribute equal to NA

```{r rateo for NonMedical MFAs 2}
na_nm_mfa_bro / fd_nm_mfa_bro
```
And from here we can see that about the 78% of the observations that are NonMedical - MFAs that have at least one district column attribute to NA are from the RICHMOND / STATEN ISLAND.
Also BRONX has about half of the NonMedical - MFAs observations that have at least one district column to NA.



### Checking the assigned units predictors

```{r check is.na assigned units}
print(fire_data %>%
  filter(is.na(engines_assigned) | is.na(ladders_assigned) | is.na(others_units_assigned)) %>%
  group_by(inc_borough, inc_class)) %>%
  summarise(incident_count = n())
```

We can easily remove this observations.

### Checking the first_act_datetime predictors

```{r is.na first_act_datetime}
na_first_act_datetime <- fire_data %>% filter(is.na(first_act_datetime))
```

```{r print is.na first_act_datetime}
print(na_first_act_datetime %>% group_by(inc_class, inc_borough) %>% summarise(incident_count = n()))
```


```{r geom_bar is.na first_act_datetime}
ggplot(data=na_first_act_datetime %>% 
        group_by(inc_class_group, inc_borough) %>%
        summarise(incident_number = n()), 
       aes(x=inc_borough, y=incident_number, fill=inc_class_group)) + geom_bar(stat="identity", position=position_dodge()) +
labs(title = "NA First Act Date", x = "Borough", y = "Incident Count", fill = "Incident Class Group") +
  theme_minimal()

```

Seems to be random and thus there is no pattern that motivate the presence of NA values in `first_act_datetime`.


**At this point we can omit the NA values.**

```{r remove na from fire_data}
fire_data_new <- na.omit(fire_data)
```


**And the un-usefull predictors**

```{r remove predecitors}
fire_data_new <- fire_data_new %>% select(-c(zipcode, pol_prec, city_con_dist, commu_dist, al_location,
                                             commu_sc_dist, cong_dist, first_ass_datetime, first_act_datetime,
                                             first_onscene_datetime, inc_close_datetime, inc_resp_min_indc, 
                                             id, al_number, inc_class,))
                                             #tua_is_one, total_assigned_unit)) 
```


```{r print ceaned datset}
print(dfSummary(fire_data, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE),
                method = 'render')
```



## Additional Data Visaulization

In this section we will have a look on additional data visualisation in order to better understand how the predictors behaves.


```{r}
summary(fire_data_new)
```


```{r}
ggplot(fire_data_new,
       aes(x = inc_borough, y = inc_resp_min_qy, color = inc_borough)) +  # ggplot function
  geom_boxplot()# +
  #labs(title = "Borough - Incident Number - Alarm Source", x = "Borough", y = "Incident Number", color = "Alarm Source")
```


```{r}
ggplot(fire_data_new,
       aes(x = al_source_desc, y = inc_resp_min_qy, color = al_source_desc)) +  # ggplot function
  geom_boxplot()# +
  #labs(title = "Borough - Incident Number - Alarm Source", x = "Borough", y = "Incident Number", color = "Alarm Source")
```

```{r}
ggplot(fire_data_new,
       aes(x = day_type, y = inc_resp_min_qy, color = day_type)) +  # ggplot function
  geom_boxplot()# +
  #labs(title = "Borough - Incident Number - Alarm Source", x = "Borough", y = "Incident Number", color = "Alarm Source")
```

```{r}
ggplot(fire_data_new,
       aes(x = time_of_day, y = inc_resp_min_qy, color = time_of_day)) +  # ggplot function
  geom_boxplot()# +
  #labs(title = "Borough - Incident Number - Alarm Source", x = "Borough", y = "Incident Number", color = "Alarm Source")
```


### Maps Visualization

In this section we plot additional data visualization focus on the geographical visualization of the New York borough with relative predictors. In order to do so we load an additional datasets:

The **fdny-firehouse-listing.csv** is a dataset that includes the geographical informations of every firefighter stations in the NYC, including again *latitude* and *longitude*.


```{r read fdny-firehouse-listing.csv}
firefighter_stations <- read.csv("datasets/fdny-firehouse-listing.csv")

head(firefighter_stations)
```

```{r sumamry firefighter_stations}
summary(firefighter_stations)
```


We now start with the firefighter stations dataset. By first making a copy of the `fire_data_new` and setting the borough from the `firefighter_stations` dataset to factor in order to be easily merged with the copied fire_data dataset.

```{r factors firefighter_stations}

# make a copy of the fire_data
fire_data_for_ffs <- fire_data_new

fire_data_for_ffs <- fire_data_for_ffs %>% rename(borough = inc_borough)

firefighter_stations$Borough <- as.factor(firefighter_stations$Borough)
firefighter_stations <- firefighter_stations %>% rename(borough = Borough)

# remove the na values from firefighter_stations
firefighter_stations <- na.omit(firefighter_stations)
```

Now we want to get the number of firefighter station for each borough.

```{r firefighter station per borough}
stations_borough <- firefighter_stations %>%
                    group_by(borough) %>%
                    summarise(number_of_stations = n())
```

Now we want to get a summary of the incident count, the number of station and the incident per station of each borough in order to have a general view of the New York City situation.

```{r incident_per_station}
count_inc_brough <- fire_data_for_ffs %>% group_by(borough) %>% summarise(incident_count = n())

stations_borough$incident_per_station <- round(count_inc_brough$incident_count / stations_borough$number_of_stations, digits = 3)

count_inc_brough <- merge(count_inc_brough, stations_borough, by="borough")

count_inc_brough
```

Now we convert the `firefighter_station` data frame into a **Spartial Data Frame** to contains the geometry points.
```{r firefighter_stations_sdf}
firefighter_stations_sdf <- st_as_sf(firefighter_stations, coords = c("Longitude", "Latitude"), crs = 4326)
head(firefighter_stations_sdf)
```

#### Downloand of the geojson file 

At this point we download the `.geojson` file that contain all the geometry of each borough in order to have a cool maps visualization of NYC.

```{r geojson_newyork}
geojson_newyork <- geojson_read("datasets/NYC_BoroughBoundaries.geojson",  what = "sp")
geojson_newyork <- setNames(geojson_newyork, c("borough_code", "borough", "shape_area", "shape_leng"))
geojson_newyork$borough <- as.factor(geojson_newyork$borough)
geojson_newyork$borough_code <- NULL
head(geojson_newyork)
```

And now we merge `geojson_newyork` with `count_inc_brough` maintaining the **Spartial Data Frame** type.

```{r merging geojson_newyork and count_inc_brough}
geojson_newyork@data = data.frame(geojson_newyork@data, count_inc_brough[match(geojson_newyork@data$borough, count_inc_brough$borough),])
geojson_newyork@data$borough.1 <- NULL
```

And finally we can plot the interactive map using the `mapview` function.

```{r mapview}
mapview(list(firefighter_stations_sdf, geojson_newyork),
        zcol = list(NULL, "incident_count"),
        legend = list(FALSE, TRUE),
        homebutton = list(FALSE, TRUE), layer.name = list(NULL, "indicents_number"), alpha.regions = 0.5, aplha = 1)
```


# Let's build some models


As suggested by the professor we have opted to solve a regression problem with response first `inc_resp_min_qy`, then the `emergency_min_qy` and finally the `ticket_time`. Previously we were thinking to solve a multi-classification / binary classification problem for the `inc_class_group`, however we were considering all the time difference predictors that are a future information w.r.t. the `inc_class_group` in prediction time, so it doesn't make much sense to use them, and it is possible also that they will result in super predictors. That's way we decided to grab the professor suggestion.

For all three analysis we transform the relative response in log scale in order to simulate the behaviour of the Exponential and Gamma GLMs.

So first things first let's check if there are some observations that have at least one of the time differences equal to zero.

```{r}
summary(fire_data_new %>% select(disp_resp_min_qy, inc_travel_min_qy, inc_resp_min_qy, emergency_min_qy, ticket_time))
```

```{r}
fire_data_new <- fire_data_new %>% filter(inc_travel_min_qy != 0, emergency_min_qy != 0)
```


But before creating any model have to split the cleaned dataset into *train* and *test*, with 0.8% of the whole dataset for the train set and the remaining 20% for the test set.

```{r}
#set.seed(43)

split <- sample.split(fire_data_new, SplitRatio = 0.8)

# Create training and testing sets
fire_data.train <- subset(fire_data_new, split == TRUE)
fire_data.test <- subset(fire_data_new, split == FALSE)

dim(fire_data.train)
dim(fire_data.test)
```

## Use emergency_min_qy as response

In this section we use `inc_resp_min_qy` as response, so we have to remove all the time difference predictors that are computed with one of the two datetime that comes after the incident datetime, so all the other except for our actual response.


```{r}
# make a copy of the train and test
resp_min_fd.train <- fire_data.train
resp_min_fd.test <- fire_data.test

# remove the ticket_time
resp_min_fd.train <- resp_min_fd.train %>% select(-c(disp_resp_min_qy, inc_travel_min_qy, emergency_min_qy, ticket_time, total_assigned_unit))
resp_min_fd.test <- resp_min_fd.test %>% select(-c(disp_resp_min_qy, inc_travel_min_qy, emergency_min_qy, ticket_time, total_assigned_unit))
```


We decided to remove also the `total_assigned_unit` since it is in linear relation with the three assigned units predictors since it is the sum of the three.

Let's build our first Linear Regression Model

```{r}
lm_full <- lm(inc_resp_min_qy ~ ., data = resp_min_fd.train)
summary(lm_full)
```


```{r}
qqPlot(residuals(lm_full))
```

Let's have a look of the possible power transformation of the response.

```{r}
powerTransform(lm_full)
```

The function **powerTransform** suggests to take the log-transformation of the response, we take the log transformation because the estimated value of **lambda** is very close to zero.


```{r}
lm_full_upd <- update(lm_full, log(inc_resp_min_qy) ~ .)
summary(lm_full_upd)
```

The $R^2$ has increase w.r.t. the previous model. Let's see the residual plots.

```{r}
residualPlots(lm_full_upd)
qqPlot(residuals(lm_full_upd))
influenceIndexPlot(lm_full_upd, vars = "Cook")
summary(fitted(lm_full_upd))
```


Now given the updated full model we can run four model selection that we saw during this course.

### Best Subset Regression

This technique looks through all possible regression models of all different subset sizes and look for the best of each size. So it produces a sequence of models which is the best subset for each particular size.

We specify the full number of available predictors by parameter `nvmax`, so we will get 1 subsets of each size up to 12 in this case.

```{r}
regfit.full <- regsubsets(log(inc_resp_min_qy) ~ ., data = resp_min_fd.train, nvmax = 25)
summ_regfit.full <- summary(regfit.full)
summ_regfit.full
```

```{r}
plot(summ_regfit.full$cp, xlab = "Number of variables", ylab = "Cp")
title("Best Subset - log(inc_resp_min_qy)")
min_cp <- which.min(summ_regfit.full$cp)
points(min_cp, summ_regfit.full$cp[min_cp], pch=20, col="red")
```

There is also an interesting plot method for the regsubset object

```{r}
plot(regfit.full, scale="Cp")
coef(regfit.full, min_cp)
```



Now we look into the three methods of best subset regression: **forward**, **backword** and **bidirectional elimination**.

```{r}
fw_regfit.full <- regsubsets(log(inc_resp_min_qy) ~ ., data = resp_min_fd.train, nvmax = 25, method = "forward")
summ_fw_regfit.full <- summary(fw_regfit.full)
fw_min_cp <- which.min(summ_fw_regfit.full$cp)


bk_regfit.full <- regsubsets(log(inc_resp_min_qy) ~ ., data = resp_min_fd.train, nvmax = 25, method = "backward")
summ_bk_regfit.full <- summary(bk_regfit.full)
bk_min_cp <- which.min(summ_bk_regfit.full$cp)


sw_regfit.full <- regsubsets(log(inc_resp_min_qy) ~ ., data = resp_min_fd.train, nvmax = 25, method = "seqrep")
summ_sw_regfit.full <- summary(sw_regfit.full)
sw_min_cp <- which.min(summ_sw_regfit.full$cp)
```


```{r}
par(mfrow = c(1, 3))
plot(summ_fw_regfit.full$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l')
points(fw_min_cp, summ_fw_regfit.full$cp[fw_min_cp], pch=20, col="red")

plot(summ_bk_regfit.full$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l')
points(bk_min_cp, summ_bk_regfit.full$cp[bk_min_cp], pch=20, col="red")

plot(summ_sw_regfit.full$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l')
points(sw_min_cp, summ_sw_regfit.full$cp[sw_min_cp], pch=20, col="red")
```


Now we can try to make prediction using the best model of all the three methods.

```{r}
val.errors = rep(NA, 25)

x.test = model.matrix(log(inc_resp_min_qy) ~ ., data = resp_min_fd.test)

for (i in 1:25) {
  coefi = coef(fw_regfit.full, id=i)
  pred = x.test[,names(coefi)]%*%coefi
  val.errors[i] = mean((log(resp_min_fd.test$inc_resp_min_qy) - pred)^2)
}

plot(sqrt(val.errors), ylab="Root MSE", ylim=c(0.368, 0.39), pch=19, type="b")
points(sqrt(fw_regfit.full$rss[-1]/dim(resp_min_fd.train)[1]), col="blue", pch=19,type="b")
legend("topright", legend=c("Training", "Testing"), col=c("blue", "black"), pch=19)
```

```{r}
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(from, newdata)
  coefi <- coef(object, id=id)
  mat[, names(coefi)]%*%coefi
}
```



### Lasso Regression


### Ridge Regression



Interestingly we see that the two time differences that we decided to add do are not significant for the moment. let's try to convert also those in logarithm scale and see if we gain something in terms of $R^2$ and $Adjusted-R^2$.


```{r}
#lm1_upd <- update(lm1, . ~ . - inc_resp_min_qy - inc_travel_min_qy + log(inc_resp_min_qy) + log(inc_travel_min_qy))
#summary(lm1_upd)
```

As expected the p-value for the two time differences now on log scale results to be significative. However the $R^2$ and $Adjusted-R^2$ have been changed only by 0.003.

### Stepwise Regression

At this point we will use **regsubsets** in all its three forms: *forward* selection, *backword* selection and *bidirectional* elimination also known as "sequential replacement".



We can use function **regsubsets** for stepwise regression. Forward selection:


### Lasso


### Ridge

