---
title: "Part 3 - Binary Classification"
author: "Zuliani Riccardo"
date: "17/1/2024"
output: 
  html_document: 
    toc: true
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# REMEMBER TO CHANCGE THE WORKING DIRECTORY
setwd("C:/Users/ricca/Desktop/UNI/Magistrale/Anno3/Statistical_Inference_and_Learning/SIL Projcet/Statistical_Inference_Learning_Project/Fire_Incident_Dispatch_Analysis")
set.seed(11)
```

# Load Libraries

```{r}
# installation of packages in case the user has not installed yet
requirements <- c("nnet", "MASS", "e1071", "class", "leaps", "glmnet", "car", "caTools",
              "pROC", "summarytools", "dplyr", "ggplot2", "tidyverse", "lubridate",
              "mapview", "sf", "geojsonio", "leaflet", "broom", "gridExtra",
              "effects")

for (req in requirements){
  if (!require(req, character.only = TRUE)){
      install.packages(req)
  }
}
```


# Cast to a Binary Classification Task

As mentioned before, we decided, as the professor suggested, to cast our regression problem into a binary classification problem by splitting the range of possible time difference responses.

We will use the same responses of the Regression Section so first `inc_resp_min_qy` and then `emergency_min_qy`. 

First things first load the cleaned train and test datasets.
```{r, cache=TRUE}
fire_data_clean.train <- read.csv("datasets/fire_data_clean.train.csv", 
                                  stringsAsFactors = TRUE)
fire_data_clean.test <- read.csv("datasets/fire_data_clean.test.csv", 
                                 stringsAsFactors = TRUE)

fire_data_clean.train$cong_dist <- as.factor(fire_data_clean.train$cong_dist)
fire_data_clean.test$cong_dist <- as.factor(fire_data_clean.test$cong_dist)

fire_data_clean <- rbind(fire_data_clean.train, fire_data_clean.test)

head(fire_data_clean)

dim(fire_data_clean)
```

Then we have to decide the range of value for both of response. 

```{r}
summary(fire_data_clean$inc_resp_min_qy)
```

```{r}
summary(fire_data_clean$emergency_min_qy)
```

As responses threshold we decide to use the third quantile of both responses, thus:

```{r}
# threshold for inc_resp_min_qy
th_irm <- summary(fire_data_clean$inc_resp_min_qy)[5]

# threshold for emergency_min_qy
th_eme <- summary(fire_data_clean$emergency_min_qy)[5]
```


# Range of inc_resp_min_qy as response

```{r}
# make a copy of the train and test
cl_resp_min_fd.train <- fire_data_clean.train
cl_resp_min_fd.test <- fire_data_clean.test

# set the factorial response via the early computed threshold
cl_resp_min_fd.train$fast_response <- cl_resp_min_fd.train$inc_resp_min_qy < th_irm
cl_resp_min_fd.test$fast_response <- cl_resp_min_fd.test$inc_resp_min_qy < th_irm

# remove the future time differences
cl_resp_min_fd.train <- cl_resp_min_fd.train %>% select(
  -c(emergency_min_qy, inc_resp_min_qy))

cl_resp_min_fd.test <- cl_resp_min_fd.test %>% select(
  -c(emergency_min_qy, inc_resp_min_qy))
```


See the summary of both train and test response.
```{r}
print(dfSummary(cl_resp_min_fd.train$fast_response, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE,
                na.col       = FALSE),
                method = 'render')
```

```{r}
print(dfSummary(cl_resp_min_fd.test$fast_response, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE,
                na.col       = FALSE),
                method = 'render')
```

As expected we have an unbalanced case, since exactly 3/4 of the train and test dataset have a fast response, whereas the rest have a low response.

## Logistic Regression

Fit our full **Logistic Regression** model.

```{r}
res_glm.full <- glm(fast_response ~ ., data = cl_resp_min_fd.train, 
                    family = "binomial")
summary(res_glm.full)
```

So by the summary we can say that:

- **inc_borough** is strongly significant for Staten Island and Manhattan and significant for Queens.
- **cong_dist** is strongly significant for the congressional districts 5 and 12, whereas it is slightly significant for 14 and 16.
- **engined_assigned** and **others_units_assigned** are strongly significant but not **ladders_assigned**.
- Interestingly the **inc_class_group** is significant only for the structural and non structural fires, maybe because these emergency are the worse.
- **al_source_desc** and **al_index_desc** have all categorical predictors poorly or slightly statistically significant.
- **working_hour**,  **time_of_day** and **tua_is_one** have pretty much all the categorical predictors strongly significant 


Let's check if we have some **multicollinearity** problem using the `vif` function as we did on the previous analysis part.

```{r}
vif(res_glm.full)
```

And update the model removing the predictors that are explained with a linear combination of other predictors. We name this new model the cleaned one since it will contain only the significant predictors.

```{r}
res_glm.clean <- update(res_glm.full, . ~ . - cong_dist - highest_al_level - inc_class_group - ladders_assigned , data = cl_resp_min_fd.train)
vif(res_glm.clean)
```

Let's see the summary of the updated cleaned model.
```{r}
summary(res_glm.clean)
```
Compare the **AIC** for the two models.
```{r}
AIC(res_glm.full, res_glm.clean) %>% arrange(AIC)
```



Now we decide to update the previous model by adding the following interaction terms:

- `al_source_desc : engines_assigned`
- `al_source_desc : others_units_assigned`
- `day_type : working_hour`
- `time_of_day : tua_is_one`

```{r}
res_glm.clean.inter <- update(res_glm.clean, . ~ . +
                                al_source_desc : engines_assigned +
                                al_source_desc : others_units_assigned +
                                day_type : working_hour +
                                time_of_day : tua_is_one,
                              data = cl_resp_min_fd.train)
summary(res_glm.clean.inter)
```

Regarding the interaction terms that we have added we note that:

- The interaction between `al_source_desc` and `engines_assigned` is strongly significant for all the cases.
- `al_source_desc` between `others_units_assigned ` is significant for many cases, but for others like **EMS** is poorly significant.
- `time_of_day` between `tua_is_one` is significant except for the **Morning** category.
- `day_type` between `working_hour` is strongly significant.

```{r}
AIC(res_glm.full, res_glm.clean, res_glm.clean.inter) %>% arrange(AIC)
```
And by looking at the **AIC** we can say that this model is better than the initial full model.

Now to have a complete comparison we decided to add the same interaction terms to the initial full model.

```{r}
res_glm.full.inter <- update(res_glm.full, . ~ . + 
                               al_source_desc : engines_assigned +
                               al_source_desc : others_units_assigned +
                               day_type : working_hour +
                               time_of_day : tua_is_one, 
                             data = cl_resp_min_fd.train)
summary(res_glm.full.inter)
```
```{r}
AIC(res_glm.full, res_glm.full.inter, res_glm.clean, res_glm.clean.inter) %>% 
  arrange(AIC)
```
It seems like that the full model with the interaction terms outperform the other specified models, later we will see if this statement holds or not during predictions.

## Model Interpretation

We decide to interpret the model with the lower AIC, so the **res_glm.full.inter** which has $24415.50$ as AIC.

For interpreting the specified model we decide to use the library `effect`. Let's recall the summary of the model, and see the effects of some strongly significant predictors.
```{r}
summary(res_glm.full.inter)
```

We analyse the effects of the following predictors: `inc_borough`, `inc_class_group`, `others_units_assigned`, and `al_source_desc`.
```{r}
plot( effect("inc_borough", res_glm.full.inter), rescale.axis = FALSE, 
      ylab = "Probability of Fast Response")
```

The lower probability of having a fast response is for The **Bronx**, whereas the higher are in the borough of **Brooklyn** and **Staten Island**, with the difference that the first has a smaller confidence interval respect to the second.

```{r}
plot(effect("inc_class_group", res_glm.full.inter), rescale.axis = FALSE, 
     ylab = "Probability of Fast Response")
```

As expected we have higher probability to have a fast response for the **Fire** and **NonFire Emergencies** incident and lower for **Medical Emergencies** and **Medical MFAs**. 


```{r}
plot(effect("others_units_assigned", res_glm.full.inter), rescale.axis = FALSE,
     ylab = "Probability of Fast Response")
```

Interestingly we can see that the **others_units_assigned** follow a logarithmic curve. 

```{r}
plot(effect("al_source_desc", res_glm.full.inter), rescale.axis = FALSE,
     ylab = "Probability of Fast Response")
```

We can see that there is an higher probability of having a fast response if the incident has origin from the **Others** category, the one that we used to merge the categries that have low frequency. Whereas the lower probability respect the possible case of have a fast response is from the **Phone** category.


Let's the effect from a the interaction of `time_of_day` and `tua_is_one`.

```{r}
plot(effect("time_of_day:tua_is_one", res_glm.full.inter), rescale.axis=FALSE, ylab="Probability of Fast Response")
```

We can see that in the case of having more than a single unit assigned we have a higher probability in the Evening of having a fast response, similarly in the case of having a single assigned unit. Interestingly we can see that the two line plots are pretty similar, they are only vertically scaled.


## Model Comparison

Now we see if what the *AIC* suggest to us is correct in practice or not by doing predictions on the test dataset.

```{r}
res_glm.full.pred <- predict(res_glm.full, newdata = cl_resp_min_fd.test, 
                             type = "response")

res_glm.full.inter.pred <- predict(res_glm.full.inter, 
                              newdata = cl_resp_min_fd.test, type = "response")

res_glm.clean.pred <- predict(res_glm.clean, newdata = cl_resp_min_fd.test, 
                              type = "response")

res_glm.clean.inter.pred <- predict(res_glm.clean.inter, 
                              newdata = cl_resp_min_fd.test, type = "response")
```



At this point we use the `pROC` library in order to extrapolate the best threshold using the function `roc`.

```{r}
par(mfrow=c(2,2))

res_glm.full.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_glm.full.pred, plot = TRUE, 
  print.auc = TRUE, main = "res_glm.full.pred ROC curve")

res_glm.full.inter.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_glm.full.inter.pred, print.auc = TRUE,
  plot = TRUE, main = "res_glm.full.inter.pred ROC curve")

res_glm.clean.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_glm.clean.pred, plot = TRUE
  , print.auc = TRUE, main = "res_glm.clean.pred ROC curve")

res_glm.clean.inter.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_glm.clean.inter.pred, 
  print.auc = TRUE, plot = TRUE, main = "res_glm.clean.inter.pred ROC curve")
```

By the plots we can see that the Area Under the Curve (AUC) computed by the ROC curves gives almost the same results.

Now we extract from each `roc` object the best threshold in order to use it during predictions.

```{r}
res_glm.full.roc.bmetrics <- coords(res_glm.full.roc, x = "best", ret = "all")

res_glm.full.inter.roc.bmetrics <- coords(res_glm.full.inter.roc, x = "best",
                                          ret = "all")

res_glm.clean.roc.bmetrics <- coords(res_glm.clean.roc, x = "best", ret = "all")

res_glm.clean.inter.roc.bmetrics <- coords(res_glm.clean.inter.roc, x = "best",
                                           ret = "all")
```

Rename the single row for each dataframe and concatenate all in a single one.
```{r}
row.names(res_glm.full.roc.bmetrics) <- "Full Model"

row.names(res_glm.full.inter.roc.bmetrics) <- "Interaction Full Model"

row.names(res_glm.clean.roc.bmetrics) <- "Cleaned Model"

row.names(res_glm.clean.inter.roc.bmetrics) <- "Interaction Cleaned Model"

res_results <- rbind(res_glm.full.roc.bmetrics, res_glm.full.inter.roc.bmetrics,
                  res_glm.clean.roc.bmetrics, res_glm.clean.inter.roc.bmetrics)
```




Now we make a comparisons result:

1) **Specificity**
```{r}
res_results %>% select(specificity) %>% arrange(desc(specificity))
```
In terms of specificity the best model is the **Interaction Full Model**.

2) **Sensitivity**
```{r}
res_results %>% select(sensitivity) %>% arrange(desc(sensitivity))
```
Regarding the sensitivity the best model is the **Full Model**.

3) **Accuracy**
```{r}
res_results %>% select(accuracy) %>% arrange(desc(accuracy))
```
And finally for the accuracy measure the best model is the **Full Model**.

However we recall that we have an unbalanced case and for this reason model choice should happen on the one that is able to correctly classify fast response time incidents of true fast response incidents.

Recalling that the sensitivity is the ratio of how many fast response emergencies we have detected that are actually fast. So since we have an unbalanced dataset with 3/4 of the observations from the train and test dataset that are fast emergencies we choose the **Full Model** since it has the highest **sensitivity** and with specificity not worse than a random choice.

Let's have a quick look at the confusion matrix for the four models that we have specified.

1- **Full Model**
```{r}
table(preds=(res_glm.full.pred > res_glm.full.roc.bmetrics$threshold), 
      true=as.logical(cl_resp_min_fd.test$fast_response))
```

2- **Interaction Full Model**
```{r}
table(preds=(res_glm.full.inter.pred > res_glm.full.inter.roc.bmetrics$threshold),
      true=as.logical(cl_resp_min_fd.test$fast_response))
```

3- **Cleaned Model**
```{r}
table(preds=(res_glm.clean.pred > res_glm.clean.roc.bmetrics$threshold), 
      true=as.logical(cl_resp_min_fd.test$fast_response))
```

4- **Interaction Cleaned Model**
```{r}
table(preds=(res_glm.clean.inter.pred > res_glm.clean.inter.roc.bmetrics$threshold),
      true=as.logical(cl_resp_min_fd.test$fast_response))
```


Now we decide to shrink the latter model by running  **Ridge Regression** and **Lasso Regression**.

## Ridge Shrinkage Estimation
We decide to use the full model with interaction terms in order to see how the ridge is able to shrink the coefficients.

We recall that the penalty on ridge is put in the sum of squares of the coefficients thus shrinking the model coefficient toward zero by the parameter $\lambda$.

So first of all we have to create our model matrices.
```{r}
# model matrix for train
x_train <- model.matrix(formula(res_glm.full.inter), data = cl_resp_min_fd.train)
y_train <- cl_resp_min_fd.train$fast_response

# mode matrix for test
x_test <- model.matrix(formula(res_glm.full.inter), data = cl_resp_min_fd.test)
y_test <- cl_resp_min_fd.test$fast_response
```



```{r}
res_fit.ridge <- glmnet(x_train, y_train, familiy = "binomial", alpha = 0)
plot(res_fit.ridge, xvar = "lambda", label = TRUE)
```

Choose the best value of lambda via cross-validation.
```{r}
res_cv.ridge <- cv.glmnet(x_train, y_train, familiy = "binomial", alpha = 0)
plot(res_cv.ridge)
```


The value of $\lambda$ that minimises the ridge cross-validated mean square error is:

```{r}
res_cv.ridge$lambda.min
```

However, empirical experience suggests to select the simplest model whose $\lambda$ value is within one standard error from the minimum of the cross-validated mean square error:

```{r }
res_bestlam.ridge <- res_cv.ridge$lambda.1se
res_bestlam.ridge
```
However, empirical experience suggests to select the simplest model whose $\lambda$ value is within one standard error from the minimum of the cross-validated mean square error:

```{r}
plot(res_fit.ridge, xvar = "lambda")
abline(v = log(res_bestlam.ridge), lwd = 1.2, lty = "dashed")
```

We make the prediction on the test set.

```{r}
res_fit.ridge.pred <- predict(res_fit.ridge, s = res_bestlam.ridge, 
                              newx = x_test, type = "response")
```

Apply the `roc` function in order to get the best threshold value.
```{r}
res_fit.ridge.roc <- roc(y_test ~ res_fit.ridge.pred, plot = TRUE,
                         print.auc = TRUE) 
```

See the result:
```{r}
res_fit.ridge.roc.bmetrics <- coords(res_fit.ridge.roc, x="best", ret="all")
res_fit.ridge.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(res_fit.ridge.pred > res_fit.ridge.roc.bmetrics$threshold), true=as.logical(cl_resp_min_fd.test$fast_response))
```


Concatenate with the other results.
```{r}
row.names(res_fit.ridge.roc.bmetrics) <- "Inteaction Full Moldel Ridge"
res_results <- rbind(res_results, res_fit.ridge.roc.bmetrics)
```




## Lasso Shrinkage Estimation

Now is the turn of lasso and we decide to use the full model with interaction for the same reason as ridge.

In Lasso instead of the sum of squares of the coefficients we penalize the sum of absolute value of the coefficients.

```{r}
res_fit.lasso <- glmnet(x_train, y_train, familiy="binomial", alpha = 1)
plot(res_fit.lasso)
```

Again choose the best lambda via cross-validation.
```{r}
res_cv.lasso <- cv.glmnet(x_train, y_train, familiy="binomial", alpha = 1)
plot(res_cv.lasso)
```

The value of $\lambda$ that minimizes the ridge cross-validated mean square error is:

```{r}
res_cv.lasso$lambda.min
```

Again like before we take as $\lambda$ value corresponding to one standard error from the minimum of the cross-validated mean square error:

```{r }
res_bestlam.lasso <- res_cv.lasso$lambda.1se
res_bestlam.lasso
```

Like before, now visualize again the lasso estimates as a function of the logarithm of $\lambda$ and add a vertical line corresponding to **best.lambda**:

```{r }
plot(res_fit.lasso, xvar = "lambda")
abline(v = log(res_bestlam.lasso), lwd = 1.2, lty = "dashed")
```


Now we apply prediction on the test set.
```{r}
res_fit.lasso.pred <- predict(res_fit.lasso, s = res_bestlam.lasso, 
                              newx = x_test, type = "response")
```

Then we apply the `roc` function to obtain the best threshold value.
```{r}
res_fit.lasso.roc <- roc(y_test ~ res_fit.lasso.pred, plot = TRUE, 
                         print.auc = TRUE) 
```

See the result.
```{r}
res_fit.lasso.roc.bmetrics <- coords(res_fit.lasso.roc, x="best", ret="all")
res_fit.lasso.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(res_fit.lasso.pred > res_fit.lasso.roc.bmetrics$threshold), true=as.logical(cl_resp_min_fd.test$fast_response))
```

Concatenate with the other results.
```{r}
row.names(res_fit.lasso.roc.bmetrics) <- "Inteaction Full Moldel Lasso"
res_results <- rbind(res_results, res_fit.lasso.roc.bmetrics)
```




## Linear Discriminant Analysis


Now we are going to deal with **LDA** or **Linear Discriminant Analysis**. For this type of Generative Model we decide to again use the full model with interaction terms.

LDA assumes that the data is normally distributed which is no our case, however we have decided to estimate its performance in any case.


```{r}
res_lda.fit <- lda(formula(res_glm.full.inter), data = cl_resp_min_fd.train)
res_lda.fit
```

The printed output of **lda** includes:

- The a-priori probabilities of fast response
- The group means.

The a-priori probabilities correspond to the sample proportion of fast response of the train dataset.

An example of group mean is the mean fast response time of incidents in the Brooklyn borough.
```{r}
with(cl_resp_min_fd.train, 
     mean(inc_borough[fast_response == TRUE] == "Brooklyn"))
```


Now we make the prediction on the test set.
```{r}
res_lda.preds <- predict(res_lda.fit, 
                         newdata = cl_resp_min_fd.test, type = "response")
```


The ROC curve for linear discriminant analysis is:
```{r}
res_lda.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_lda.preds$posterior[, 2], 
  plot = TRUE, print.auc = TRUE)
```

The best choice for the threshold of linear discriminant analysis yields an accuracy of:
```{r}
res_lda.roc.bmetrics <- coords(res_lda.roc, x = "best", ret = "all")
res_lda.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(res_lda.preds$posterior[, 2] > res_lda.roc.bmetrics$threshold), true=as.logical(cl_resp_min_fd.test$fast_response))
```

Concatenate with the other results.
```{r}
row.names(res_lda.roc.bmetrics) <- "Linear Discriminant Analysis"
res_results <- rbind(res_results, res_lda.roc.bmetrics)
```







## Naive Baye

Continuing with the analysis we try **Naive Bayes** algorithm. In this case we can't use a model with interaction term since Naive Bayes do not support these relations. Thus we decide to use the **Full Model**.

Note that Naive Bayes assumes that all predictors are independent to each other.

```{r}
res_nb.fit <- naiveBayes(formula(res_glm.full), data = cl_resp_min_fd.train)
res_nb.fit
```


The output of **naiveBayes** contains:

- The estimated a-priori probabilities.
- The estimated conditional probabilities for the qualitative variables.
- The estimated group means and standard deviations for the quantitative variables.

The a-priori probabilities again correspond to the sample proportion of fast response of the train dataset.

An example of conditional probabilities of `al_source_desc` for fast response time are:
```{r}
with(cl_resp_min_fd.train, table(al_source_desc[fast_response == TRUE]) / 
       sum(fast_response == TRUE))
```


The mean and standard deviations of `engines_assigned` conditional to the response time speed are:
```{r}
with(cl_resp_min_fd.train, 
     mean(engines_assigned[fast_response == FALSE]))
with(cl_resp_min_fd.train, 
     sd(engines_assigned[fast_response == FALSE]))
```

```{r}
with(cl_resp_min_fd.train, 
     mean(engines_assigned[fast_response == TRUE]))
with(cl_resp_min_fd.train, 
     sd(engines_assigned[fast_response == TRUE]))
```


Prediction for the test set:
```{r}
res_nb.preds <- predict(res_nb.fit, newdata = cl_resp_min_fd.test)
table(preds = res_nb.preds, true = cl_resp_min_fd.test$fast_response)
```

Predicted class probabilities can be obtained using the argument **type = "raw"**:
```{r}
res_nb.posterior <- predict(res_nb.fit,
                            newdata = cl_resp_min_fd.test, type = "raw")
head(res_nb.posterior)
```

ROC curve:
```{r}
res_nb.roc <- roc(
  cl_resp_min_fd.test$fast_response ~ res_nb.posterior[, 2],
  plot = TRUE, print.auc = TRUE)
```

```{r}
res_nb.roc.bmetrics <- coords(res_nb.roc, x = "best", ret = "all")
res_nb.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(res_nb.posterior[, 2] > res_nb.roc.bmetrics$threshold), 
      true=as.logical(cl_resp_min_fd.test$fast_response))
```

```{r}
row.names(res_nb.roc.bmetrics) <- "Naive Bayes"
res_results <- rbind(res_results, res_nb.roc.bmetrics)
```




## Conclusion
Finally we make a recap of the final results that we have obtained.
```{r}
res_results
```


1) **Specificity**
```{r}
res_results %>% select(specificity) %>% arrange(desc(specificity))
```

For the specificity the best model is **Interaction Full Model**.

2) **Sensitivity**
```{r}
res_results %>% select(sensitivity) %>% arrange(desc(sensitivity))
```

For sensitivity the best model is **Naive Bayes**.

3) **Accuracy**
```{r}
res_results %>% select(accuracy) %>% arrange(desc(accuracy))
```
Finally regarding the accuracy measure the best model is again the **Interaction Full Model Lasso**.


In conclusion, for this analysis we have seen that more or less all the models that we have tried have similar performance and interestingly the initial model that we have specified is the one that better solves our prediction task of trying to predict if a given incident call has a fast or low response time.
However recalling the unbalanced problem our choice of the top 3 overall model is the following one:

1. **Interaction Full Model Lasso**: it has the highest accuracy, is second on the sensitivity measure which is the one that we are most interested in, and it is on the 
average ranking for specificity.
2. **Full Model** since it has the second highest accuracy at equal merit with the Naive Bayes, it is third overall regarding the sensitivity even if it is the third ultimate on specificity but it is not worse than random choice.
3. **Naive Bayes** since has the same accuracy of the Full Model but it has the best sensitivity and worst specificity.








# Use the range of emergency_min_qy as response

Now we will use the second response and perform a similar analysis.

```{r}
# make a copy of the train and test
cl_emerg_min_fd.train <- fire_data_clean.train
cl_emerg_min_fd.test <- fire_data_clean.test

cl_emerg_min_fd.train$fast_emergency <- cl_emerg_min_fd.train$emergency_min_qy < th_eme
cl_emerg_min_fd.test$fast_emergency <- cl_emerg_min_fd.test$emergency_min_qy < th_eme

# remove the future time differences and units counts
cl_emerg_min_fd.train <- cl_emerg_min_fd.train %>% select(-c(emergency_min_qy))
cl_emerg_min_fd.test <- cl_emerg_min_fd.test %>% select(-c(emergency_min_qy))
```


See the summary of both train and test response.
```{r}
print(dfSummary(cl_emerg_min_fd.test$fast_emergency, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE,
                na.col       = FALSE),
                method = 'render')
```

```{r}
print(dfSummary(cl_emerg_min_fd.test$fast_emergency, 
                plain.ascii  = FALSE, 
                style        = "multiline", 
                headings     = FALSE,
                graph.magnif = 0.8, 
                valid.col    = FALSE,
                na.col       = FALSE),
                method = 'render')
```




## Logistic Regression

Fit our full **Logistic Regression** model.

```{r}
eme_glm.full <- glm(fast_emergency ~ ., data = cl_emerg_min_fd.train, 
                    family = "binomial")
summary(eme_glm.full)
```

So by the summary we can say that:

- **inc_borough** is strongly significant except for Manhattan.
- **cong_dist** is significant only for the congressional district 13.
- **al_source_desc** are all very strongly significant.
- **al_index_desc** is strongly significant only for the categorical value *Initial Alarm*.
- **engined_assigned** and **others_units_assigned** are strongly significant but not **ladders_assigned**.
- **inc_class_group** is strongly significant for about every categorical value.
- **inc_resp_min_qy** is strongly significant.
- Differently from the previous analysis **working_hour** is not significant.
- **time_of_day** and **tua_is_one** are strongly significant.


Let's check if we have some **multicollinearity** problem using the `vif` function as we did on the previous analysis part.

```{r}
vif(eme_glm.full)
```

Now we remove the non significant predictor building as before the cleaned model.

```{r}
eme_glm.clean <- update(eme_glm.full, . ~ . - cong_dist - highest_al_level - 
                          inc_class_group - day_type - working_hour, 
                        data = cl_emerg_min_fd.train)
vif(eme_glm.clean)
```

Let's see the summary of the updated cleaned model.
```{r}
summary(eme_glm.clean)
```
Compare the **AIC** for the two models.
```{r}
AIC(eme_glm.full, eme_glm.clean) %>% arrange(AIC)
```

The cleaned model appears to have a higher AIC thus up to now we prefer the full initial model.

Now we decide to update the previous model by adding an interaction term for each type of assigned units with the alarm source description.

```{r}
eme_glm.clean.inter <- update(eme_glm.clean, . ~ . +
                                al_source_desc : engines_assigned +
                                al_source_desc : others_units_assigned +
                                al_source_desc : ladders_assigned,
                              data = cl_emerg_min_fd.train)
summary(eme_glm.clean.inter)
```

```{r}
AIC(eme_glm.full, eme_glm.clean, eme_glm.clean.inter) %>% arrange(AIC)
```

By looking at the AIC the best model remain the initial full model.

Now to have a complete comparison we decided to add the same interaction terms to the initial full model.

```{r}
eme_glm.full.inter <- update(eme_glm.full, . ~ . + 
                               al_source_desc : engines_assigned +
                               al_source_desc : others_units_assigned +
                               al_source_desc : ladders_assigned,
                             data = cl_emerg_min_fd.train)
summary(eme_glm.full.inter)
```


```{r}
AIC(eme_glm.full, eme_glm.full.inter, eme_glm.clean, eme_glm.clean.inter) %>% arrange(AIC)
```

And it seems like that the full model with the interaction is just slightly better with respect to the full model without interaction terms.


## Model Interpretation

We decide to interpret the model with the lower AIC, so the **eme_glm.full.inter** which has $24167.10$ as AIC.

For interpreting the specified model we decide to use the library `effect`. Let's recall the summary of the model, and see the effects of some strongly significant predictors.
```{r}
summary(eme_glm.full.inter)
```

We analyse the effects of the following predictors: `inc_borough`, `inc_class_group`, `al_source_desc`, and `inc_resp_min_qy`.
```{r}
plot(effect("inc_borough", eme_glm.full.inter), rescale.axis = FALSE, 
     ylab = "Probability of Fast Emergency")
```

The higher probability of having a fast emergency is for the borough of **Brooklyn**, whereas the lower probability is for both **Bronx** and **Manhattan** having also a pretty big confidence interval.

```{r}
plot(effect("inc_class_group", eme_glm.full.inter), rescale.axis = FALSE, 
     ylab = "Probability of Fast Emergency")
```

As expected we have a lower probability to have a fast emergency for the and **NonFire Emergencies** and **Medical Emergencies**. On the other hand we have a higher probability of having a fast emergency for the **Medical MFAs** even if there is a big confidence interval.


```{r}
plot(effect("al_source_desc", eme_glm.full.inter), rescale.axis = FALSE, 
     ylab = "Probability of Fast Emergency")
```

We have the highest probability of having a fast emergency for the category **CLASS-3** with small confidence interval, whereas the lowest is from the **Others**.

```{r}
plot(effect("inc_resp_min_qy", eme_glm.full.inter), rescale.axis = FALSE, 
     ylab = "Probability of Fast Emergency")
```

As expected having a quick response time is much likely to have higher probability of fast emergency, whereas slow response means low probability of having a fast emergency.


## Model Comparison

Now we see if what the *AIC* suggest to us is correct in practice or not by doing predictions on the test dataset.

```{r}
eme_glm.full.pred <- predict(eme_glm.full, newdata = cl_emerg_min_fd.test, 
                             type = "response")

eme_glm.full.inter.pred <- predict(eme_glm.full.inter, newdata = cl_emerg_min_fd.test, 
                             type = "response")

eme_glm.clean.pred <- predict(eme_glm.clean, newdata = cl_emerg_min_fd.test, 
                              type = "response")

eme_glm.clean.inter.pred <- predict(eme_glm.clean.inter, newdata = cl_emerg_min_fd.test, 
                              type = "response")
```



At this point we use the `pROC` library in order to extrapolate the best threshold using the function `roc`.

```{r}
par(mfrow=c(2,2))

eme_glm.full.roc <- roc(
  cl_emerg_min_fd.test$fast_emergency ~ eme_glm.full.pred, plot = TRUE,
  print.auc = TRUE, main = "res_glm.full.pred ROC curve")

eme_glm.full.inter.roc <- roc(
  cl_emerg_min_fd.test$fast_emergency ~ eme_glm.full.inter.pred, plot = TRUE,
  print.auc = TRUE, main = "res_glm.full.pred ROC curve")

eme_glm.clean.roc <- roc(
  cl_emerg_min_fd.test$fast_emergency ~ eme_glm.clean.pred, plot = TRUE, 
  print.auc = TRUE, main = "res_glm.clean.pred ROC curve")

eme_glm.clean.inter.roc <- roc(
  cl_emerg_min_fd.test$fast_emergency ~ eme_glm.clean.inter.pred, plot = TRUE, 
  print.auc = TRUE, main = "res_glm.clean.pred ROC curve")
```

By the plots we can see that the Area Under the Curve (AUC) computed by the ROC curves gives almost the same results.

Now we extract from each `roc` object the best threshold in order to use it during predictions.

```{r}
eme_glm.full.roc.bmetrics <- coords(eme_glm.full.roc, x = "best", ret = "all")

eme_glm.full.inter.roc.bmetrics <- coords(eme_glm.full.inter.roc, x = "best", ret = "all")

eme_glm.clean.roc.bmetrics <- coords(eme_glm.clean.roc, x = "best", ret = "all")

eme_glm.clean.inter.roc.bmetrics <- coords(eme_glm.clean.inter.roc, x = "best", ret = "all")
```

Rename the single row for each dataframe and concatenate all in a single one.
```{r}
row.names(eme_glm.full.roc.bmetrics) <- "Full Model"
row.names(eme_glm.full.inter.roc.bmetrics) <- "Interaction Full Model"
row.names(eme_glm.clean.roc.bmetrics) <- "Cleaned Model"
row.names(eme_glm.clean.inter.roc.bmetrics) <- "Interaction Cleaned Model"

eme_results <- rbind(eme_glm.full.roc.bmetrics, eme_glm.full.inter.roc.bmetrics, eme_glm.clean.roc.bmetrics, eme_glm.clean.inter.roc.bmetrics)
```


Now we make a comparisons result:

1) **Specificity**
```{r}
eme_results %>% select(specificity) %>% arrange(desc(specificity))
```
In terms of specificity the best model is the **Interaction Full Model**.

2) **Sensitivity**
```{r}
eme_results %>% select(sensitivity) %>% arrange(desc(sensitivity))
```
Regarding the sensitivity the best model is the **Cleaned Model**.

3) **Accuracy**
```{r}
eme_results %>% select(accuracy) %>% arrange(desc(accuracy))
```
And finally for the accuracy measure the best model is the **Cleaned Model**.

However as initially saw we have an unbalanced problem, so for the same reasons of the previous analysis we decide to opt for the **Cleaned Model**. 


Let's have a quick look on the confusion matrix for the four model that we have specified.

1- **Full Model**
```{r}
table(preds=(eme_glm.full.pred > eme_glm.full.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```

2. **Interaction Full Model**
```{r}
table(preds=(eme_glm.full.inter.pred > eme_glm.full.inter.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```

3- **Cleaned Model**
```{r}
table(preds=(eme_glm.clean.pred > eme_glm.clean.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```

4- **Interaction Cleaned Model**
```{r}
table(preds=(eme_glm.clean.inter.pred > eme_glm.clean.inter.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```



Now we decide to Shrinkage the latter model by running  **Ridge Regression** and **Lasso Regression**.




## Ridge Shrinkage Estimation
We decide to use the interaction full model in order to see how ridge is able to shrinkage the coefficients.

First of all we have to create our model matrices.
```{r}
# model matrix for train
x_train <- model.matrix(formula(eme_glm.full.inter), data = cl_emerg_min_fd.train)
y_train <- cl_emerg_min_fd.train$fast_emergency

# mode matrix for test
x_test <- model.matrix(formula(eme_glm.full.inter), data = cl_emerg_min_fd.test)
y_test <- cl_emerg_min_fd.test$fast_emergency
```



```{r}
eme_fit.ridge <- glmnet(x_train, y_train, familiy = "binomial", alpha = 0)
plot(eme_fit.ridge, xvar = "lambda", label = TRUE)
```

Choose the best value of lambda via cross-validation.
```{r}
eme_cv.ridge <- cv.glmnet(x_train, y_train, familiy = "binomial", alpha = 0)
plot(eme_cv.ridge)
```


The value of $\lambda$ that minimizes the ridge cross-validated mean square error is:

```{r}
eme_cv.ridge$lambda.min
```


However, empirical experience suggests to select the simplest model whose $\lambda$ value is within one standard error from the minimum of the cross-validated mean square error:

```{r }
eme_bestlam.ridge <- eme_cv.ridge$lambda.1se
eme_bestlam.ridge
```
Now visualize again the ridge estimates as a function of the logarithm of $\lambda$ and add a vertical line corresponding to **best.lambda**:

```{r}
plot(eme_fit.ridge, xvar = "lambda")
abline(v = log(eme_bestlam.ridge), lwd = 1.2, lty = "dashed")
```

We make the prediction on the test set.

```{r}
eme_fit.ridge.pred <- predict(eme_fit.ridge, s = eme_bestlam.ridge, 
                              newx = x_test, type = "response")
```

Apply the `roc` function in order to get the best threshold value.
```{r}
eme_fit.ridge.roc <- roc(y_test ~ eme_fit.ridge.pred, plot = TRUE,
                         print.auc = TRUE) 
```

See the result:
```{r}
eme_fit.ridge.roc.bmetrics <- coords(eme_fit.ridge.roc, x="best", ret="all")
eme_fit.ridge.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(eme_fit.ridge.pred > eme_fit.ridge.roc.bmetrics$threshold), true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```


Concatenate with the other results.
```{r}
row.names(eme_fit.ridge.roc.bmetrics) <- "Full Moldel Ridge"
eme_results <- rbind(eme_results, eme_fit.ridge.roc.bmetrics)
```




## Lasso Shrinkage Estimation

Now is the turn of lasso and we decide to use again the full model for the same reason of ridge.

```{r}
eme_fit.lasso <- glmnet(x_train, y_train, familiy="binomial", alpha = 1)
plot(eme_fit.lasso)
```

Again choose the best lambda via cross-validation.
```{r}
eme_cv.lasso <- cv.glmnet(x_train, y_train, familiy="binomial", alpha = 1)
plot(eme_cv.lasso)
```


The value of $\lambda$ that minimizes the ridge cross-validated mean square error is:

```{r}
eme_cv.lasso$lambda.min
```

Again like before we take as $\lambda$ value corresponding to one standard error from the minimum of the cross-validated mean square error:

```{r }
eme_bestlam.lasso <- eme_cv.lasso$lambda.1se
eme_bestlam.lasso
```


Like before, now visualize again the lasso estimates as a function of the logarithm of $\lambda$ and add a vertical line corresponding to **best.lambda**:

```{r }
plot(eme_fit.lasso, xvar = "lambda")
abline(v = log(eme_bestlam.lasso), lwd = 1.2, lty = "dashed")
```


Now we apply prediction on the test set.
```{r}
eme_fit.lasso.pred <- predict(eme_fit.lasso, s = eme_bestlam.lasso, 
                              newx = x_test, type = "response")
```

Then we apply the `roc` function to obtain the best threshold value.
```{r}
eme_fit.lasso.roc <- roc(y_test ~ eme_fit.lasso.pred, plot = TRUE, 
                         print.auc = TRUE) 
```

See the result.
```{r}
eme_fit.lasso.roc.bmetrics <- coords(eme_fit.lasso.roc, x="best", ret="all")
eme_fit.lasso.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(eme_fit.lasso.pred > eme_fit.lasso.roc.bmetrics$threshold), true=as.logical(cl_resp_min_fd.test$fast_response))
```

Concatenate with the other results.
```{r}
row.names(eme_fit.lasso.roc.bmetrics) <- "Full Moldel Lasso"
eme_results <- rbind(eme_results, eme_fit.lasso.roc.bmetrics)
```




## Linear Discriminant Analysis


Now we are going to deal with **LDA** or **Linear Discriminant Analysis**. For this type of Generative Model we decide to again to use the interaction full model.

LDA assumes that the data is normally distributed which is no our case, however we have decided to estimate its performance in any case.


```{r}
eme_lda.fit <- lda(formula(eme_glm.full.inter), data = cl_emerg_min_fd.train)
eme_lda.fit
```


An example of group mean is the mean engines assigned to slow emergencies.
```{r}
with(cl_emerg_min_fd.train, 
     mean(engines_assigned[fast_emergency == FALSE]))
```



Now we make the prediction on the test set.
```{r}
eme_lda.preds <- predict(eme_lda.fit, newdata = cl_emerg_min_fd.test, 
                         type = "response")
```


The ROC curve for linear discriminant analysis is:
```{r}
eme_lda.roc <- roc(
  cl_emerg_min_fd.test$fast_emergency ~ eme_lda.preds$posterior[, 2],
  plot = TRUE, print.auc = TRUE)
```

The best choice for the threshold of linear discriminant analysis yields an accuracy of:
```{r}
eme_lda.roc.bmetrics <- coords(eme_lda.roc, x = "best", ret = "all")
eme_lda.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(eme_lda.preds$posterior[, 2] > eme_lda.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```

Concatenate with the other results.
```{r}
row.names(eme_lda.roc.bmetrics) <- "Linear Discriminant Analysis"
eme_results <- rbind(eme_results, eme_lda.roc.bmetrics)
```







## Naive Baye

Continuing with the analysis we try **Naive Bayes** algorithm. In this case we can't use a model with interaction term since Naive Bayes do not support these relations. Thus we decide to use the **Full Model**.

Note that Naive Bayes assumes all predictors being independent to each other.

```{r}
eme_nb.fit <- naiveBayes(formula(eme_glm.full), data = cl_emerg_min_fd.train)
eme_nb.fit
```


An example of conditional probabilities of `al_index_desc` for slow response time are:
```{r}
with(cl_emerg_min_fd.train, table(al_index_desc[fast_emergency == FALSE]) / 
       sum(fast_emergency == FALSE))
```


The mean and standard deviations of `inc_resp_min_qy` conditional to the emergency time taken are:
```{r}
with(cl_emerg_min_fd.train, 
     mean(inc_resp_min_qy[fast_emergency == FALSE]))
with(cl_emerg_min_fd.train, 
     sd(inc_resp_min_qy[fast_emergency == FALSE]))
```

```{r}
with(cl_emerg_min_fd.train, 
     mean(inc_resp_min_qy[fast_emergency == TRUE]))
with(cl_emerg_min_fd.train, 
     sd(inc_resp_min_qy[fast_emergency == TRUE]))
```



Prediction for the test set:
```{r}
eme_nb.preds <- predict(eme_nb.fit, newdata = cl_emerg_min_fd.test)
```

Predicted class probabilities can be obtained using the argument **type = "raw"**:
```{r}
eme_nb.posterior <- predict(eme_nb.fit, newdata = cl_emerg_min_fd.test,
                            type = "raw")
head(eme_nb.posterior)
```

ROC curve:
```{r}
eme_nb.roc <- roc(cl_emerg_min_fd.test$fast_emergency ~ eme_nb.posterior[, 2], 
                  plot = TRUE, print.auc = TRUE)
```

```{r}
eme_nb.roc.bmetrics <- coords(eme_nb.roc, x = "best", ret = "all")
eme_nb.roc.bmetrics
```

See the confusion matrix using the best threshold value.
```{r}
table(preds=(eme_nb.posterior[, 2] > eme_nb.roc.bmetrics$threshold), 
      true=as.logical(cl_emerg_min_fd.test$fast_emergency))
```

```{r}
row.names(eme_nb.roc.bmetrics) <- "Naive Bayes"
eme_results <- rbind(eme_results, eme_nb.roc.bmetrics)
```



## Conclusion
Finally we make a recap of the final results that we have obtained.
```{r}
eme_results
```


1) **Specificity**
```{r}
eme_results %>% select(specificity) %>% arrange(desc(specificity))
```

For the specificity the best model is **Naive Bayes**.

2) **Sensitivity**
```{r}
eme_results %>% select(sensitivity) %>% arrange(desc(sensitivity))
```

For sensitivity the best model is **Cleaned Model**.

3) **Accuracy**
```{r}
eme_results %>% select(accuracy) %>% arrange(desc(accuracy))
```

Finally regarding the accuracy measure the best model is again the **Cleaned Model**.

In conclusion we say that the best model for this analysis is the **Cleaned Model**, so what we have discussed previously remains true. To recall we opt for this model since we are again in an unbalanced case regarding the response, thus we have to prefer a model that is able to correctly predict fast emergencies being those emergencies actually fast (so high sensitivity). But of course maintaining an eye on the specificity measure.

# Special Thanks 
If you are arrived up here, I would like to thank you for your time, I hope that the analysis were interesting to read.




