
# Molutinomial Logistic Regression
```{r}
pValue_extract <- function(x){
  z <- summary(x)$coefficients/summary(x)$standard.errors
  # 2-tailed Wald z tests to test significance of coefficients
  p <- (1 - pnorm(abs(z), 0, 1)) * 2
  p
}
```


```{r}
multi.fit1 <- multinom(incident_class ~ ., data = fire_data.train, trace = FALSE)
```

```{r}
table(preds = multi.preds1 <- predict(multi.fit1, newdata = fire_data.test), true = fire_data.test$incident_class)
```

```{r}
multi.fit1$AIC
```
```{r}
mean(multi.preds1 == fire_data.test$incident_class)
```

```{r}
#pValue_extract(multi.fit1)
```




Try a new model with less predictors
```{r}
multi.fit2 <- update(multi.fit1, . ~ . - AL_loction - zip - Pprecint - citycouncil_dist - community_dist - comm_school_dist - congressional_dist - day_number)
multi.fit2$AIC
```

```{r}
table(preds = multi.preds2 <- predict(multi.fit2, newdata = fire_data.test), true = fire_data.test$incident_class)
```


```{r}
mean(multi.preds2 == fire_data.test$incident_class)
```

```{r}
summary(multi.fit2, wald = TRUE)
```

```{r}
#pValue_extract(multi.fit2)
```


# Linear Discriminant Analysis
Now consider linear discriminant analysis:
```{r}
library("MASS")
lda.fit <- lda(formula(multi.fit2), data = fire_data.train)
lda.fit
```
Prediction:
```{r}
lda.preds <- predict(lda.fit, newdata = fire_data.test)
``` 

```{r}
table(preds = lda.preds$class, true = fire_data.test$incident_class)
```
The accuracy of linear discriminant analysis is worse respect to the multinomial logistic regression:
```{r}
mean(lda.preds$class == fire_data.test$incident_class)
```


Now consider quadratic discriminant analysis:



# Naive Bayes
```{r}
library("e1071")
nb.fit <- naiveBayes(formula(multi.fit2), data = fire_data.train)
```
Prediction:
```{r}
nb.preds <- predict(nb.fit, newdata = fire_data.test, type = "class")
``` 

```{r}
table(preds = nb.preds, true = fire_data.test$incident_class)
```
The accuracy of linear discriminant analysis is worse respect to the multinomial logistic regression:
```{r}
mean(nb.preds == fire_data.test$incident_class)
```

# K-Nearest Neighbours
```{r}
x.train <- model.matrix(~ incident_brough + alarm_source_desc + alarm_level_idx + 
    highest_alarm_lev + incident_resp_sec + incident_travel_time_sec + 
    engines_assigned + ladders_assigned + other_utits_assigned + 
    time_of_day, data = fire_data.train)[, -1]
x.test <- model.matrix(~ incident_brough + alarm_source_desc + alarm_level_idx + 
    highest_alarm_lev + incident_resp_sec + incident_travel_time_sec + 
    engines_assigned + ladders_assigned + other_utits_assigned + 
    time_of_day, data = fire_data.test)[, -1]
library("class")
set.seed(98765)
rates <- double(30)
for (i in 1:30) {
  tmp <- knn(train = x.train, test = x.test, cl = fire_data.train$incident_class, k = i)
  rates[i] <- mean(tmp == fire_data.test$incident_class)
}
plot(x = (1:30), y = rates, xlab = "k", ylab = "Accuracy", type = "l")
```


